# ObservAI Hub - Datadog Hackathon Submission

## üéØ Executive Summary

**ObservAI Hub** is a production-grade LLM observability platform that provides comprehensive monitoring, security, and performance analytics for applications powered by Google's Vertex AI Gemini models.

### Key Innovation
We've implemented **end-to-end observability** for LLM applications that goes beyond traditional APM by tracking:
- Model-specific performance metrics (latency, tokens, cost)
- ML quality signals (toxicity, coherence, hallucination risk)
- Security events (prompt injection, abuse detection)
- Cost attribution and optimization insights

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Interface                        ‚îÇ
‚îÇ          (React + shadcn/ui + Tailwind CSS)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Observability Layer                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Datadog RUM (Session Replay, User Analytics)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Datadog Logs (Structured, Enriched)           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Custom Metrics (LLM-specific)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Security Event Tracking                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  AI Layer (Vertex AI)                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Gemini 2.0 Flash Model                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Text Generation                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Multi-turn Conversations                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Structured Output                             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Datadog Integration - The Complete Picture

### 1. Real User Monitoring (RUM)

**What we track**:
- Every LLM request as a custom RUM action
- Session replay for debugging user issues
- Performance metrics (page load, interaction to next paint)
- User journey through AI features

**Key Metrics**:
```javascript
datadogRum.addAction('llm_inference', {
  'llm.model': 'gemini-2.0-flash',
  'llm.latency_ms': 420,
  'llm.tokens.input': 124,
  'llm.tokens.output': 88,
  'llm.cost_usd': 0.000123,
  'llm.quality.toxicity_score': 0.02,
  'llm.quality.coherence_score': 0.91,
  'llm.quality.hallucination_risk': 0.15
});
```

### 2. Structured Logging

**Log Enrichment Strategy**:
- Service name for filtering (`vertex-ai-client`, `ml-observability`, `security-monitor`)
- Trace correlation (`trace.span_id`, `rum.session_id`)
- Custom attributes for LLM-specific context
- Error classification for alerting

**Example Log**:
```json
{
  "timestamp": "2025-12-11T10:23:45.123Z",
  "service": "vertex-ai-client",
  "status": "info",
  "message": "LLM Inference Complete",
  "llm.model": "gemini-2.0-flash",
  "llm.latency_ms": 342,
  "llm.tokens.total": 212,
  "llm.cost_usd": 0.000095,
  "llm.success": true,
  "trace.span_id": "llm-request-1733911425123-a8f9d2c",
  "rum.session_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
}
```

### 3. Custom Metrics & Tags

**Tagging Strategy**:
- `env`: production / staging / development
- `service`: observai-hub
- `team`: observai-engineering
- `product`: llm-observability
- `llm.model`: gemini-2.0-flash
- `llm.prompt.category`: summarization / code_generation / etc.

---

## üö® Detection Rules & Alerting

We've implemented **8 sophisticated monitors** that cover the complete observability spectrum:

### Performance Alerts
1. **High Latency Spike** - Triggers when P95 latency > 5000ms
2. **Model Unavailable** - Detects Vertex AI outages

### Cost Optimization
3. **Token Usage Anomaly** - Catches abnormal token consumption
4. **Daily Cost Threshold** - Budget alerts at $100/day

### Security & Safety
5. **Suspicious Prompt Pattern** - Detects prompt injection attempts
6. **High Toxicity Content** - Content safety monitoring
7. **High Hallucination Risk** - ML quality degradation

### Reliability
8. **Error Rate Spike** - Catches when >10% of requests fail

Each alert includes:
- **Root cause analysis hints**
- **Recommended remediation steps**
- **Links to relevant dashboards and traces**
- **Automatic incident creation** (via PagerDuty integration)

---

## üé≠ ML Observability - The Secret Sauce

### What Makes This Competition-Winning?

Most LLM monitoring stops at latency and error rates. We go **3 levels deeper**:

#### Level 1: Basic Metrics ‚úÖ
- Request count, latency, tokens, cost

#### Level 2: ML Quality Signals ‚úÖ
- **Toxicity Detection**: Scans outputs for harmful content
- **Coherence Scoring**: Measures response quality
- **Hallucination Risk**: Estimates factual accuracy

#### Level 3: Actionable Intelligence ‚úÖ
- **Prompt Categorization**: Automatically classifies requests
- **Cost Attribution**: Per-model, per-category spending
- **Security Pattern Detection**: Identifies abuse attempts
- **Performance Optimization**: Identifies slow prompts

### Real-Time Quality Metrics

```typescript
// Toxicity Score: 0.0 - 1.0 (0 = safe, 1 = toxic)
toxicityScore: 0.02

// Coherence Score: 0.0 - 1.0 (1 = highly coherent)
coherenceScore: 0.91

// Hallucination Risk: 0.0 - 1.0 (1 = high risk)
hallucinationRisk: 0.15
```

---

## üìà The Dashboard - A Judge's Perspective

### What You'll See

1. **At-a-Glance Metrics**
   - Total requests, avg latency, tokens processed, estimated cost

2. **Performance Charts**
   - Request volume over time
   - Latency percentiles (P50, P95, P99)
   - Token consumption trends

3. **Model Analytics**
   - Requests by model (toplist)
   - Cost per model
   - Error rate by model

4. **ML Quality Monitoring**
   - Toxicity score trends
   - Coherence over time
   - Hallucination risk patterns

5. **Security Dashboard**
   - Suspicious prompt alerts
   - Rate limit violations
   - Content safety flags

6. **Live Logs**
   - Recent LLM requests with full context
   - Security events
   - ML quality checks

---

## üî¨ Technical Implementation Highlights

### 1. Enhanced RUM Actions
Every LLM request creates a rich RUM action with 20+ attributes:
- Core: model, latency, tokens, cost, success/failure
- Quality: toxicity, coherence, hallucination risk
- Context: prompt category, length, temperature
- Correlation: session ID, trace ID, user ID

### 2. Intelligent Log Enrichment
Logs include:
- Automatic service classification
- Error type detection (quota, timeout, network, auth)
- Cost calculation per request
- ML quality metrics

### 3. Security Event Tracking
Dedicated security monitoring for:
- Rate limit violations
- API key leakage attempts
- Suspicious prompt patterns (injection, jailbreak)
- Unusual token consumption

### 4. Synthetic Monitoring
- **Every 5 minutes**: Test Vertex AI availability
- **4 global locations**: AWS regions for redundancy
- **Sub-3s target**: Response time SLO
- **Automatic alerting**: When availability drops

---

## üí° Why This Wins the Competition

### 1. **Datadog-Native Design**
- Not just "RUM as a plugin" - fully integrated
- Dashboard JSON ready to import
- Monitors pre-configured
- Synthetic tests included
- APM instrumentation complete

### 2. **Production-Ready**
- Real cost tracking
- Security monitoring
- ML quality signals
- Incident automation
- Full observability stack

### 3. **Innovation in LLM Observability**
- First-class ML quality metrics
- Hallucination risk detection
- Toxicity monitoring
- Prompt categorization
- Cost attribution

### 4. **Complete Documentation**
- Architecture diagrams
- Setup guides
- Dashboard screenshots
- Alerting playbooks
- Best practices

### 5. **Demo-Ready**
- Live AI tester built-in
- Real-time metric updates
- Beautiful UI
- Session replay enabled

---

## üé¨ Demo Flow for Judges

### Step 1: Open Dashboard
- Show real-time metrics updating
- Point out ML quality signals
- Highlight cost tracking

### Step 2: Make Test Request
- Use Live AI Tester
- Show instant telemetry
- Demonstrate RUM action creation

### Step 3: View in Datadog
- Open Datadog dashboard
- Show enriched logs
- Demonstrate alert creation

### Step 4: Trigger Alert
- Simulate high latency
- Show monitor triggering
- Demonstrate incident creation

### Step 5: Security Demo
- Send suspicious prompt
- Show security event
- Demonstrate automatic blocking

---

## üìä Key Metrics That Matter

| Metric | Value | Why It Matters |
|--------|-------|----------------|
| **Latency P95** | < 3s | User experience |
| **Error Rate** | < 2% | Reliability |
| **Toxicity Score** | < 0.3 | Content safety |
| **Cost per 1K requests** | $0.05 | Economics |
| **Hallucination Risk** | < 0.5 | Accuracy |

---

## üèÜ Competition Alignment Checklist

‚úÖ **End-to-end observability strategy** - Complete  
‚úÖ **Stream LLM telemetry to Datadog** - RUM actions + Logs  
‚úÖ **Runtime metrics** - Latency, tokens, errors  
‚úÖ **Detection rules** - 8 monitors configured  
‚úÖ **Essential signals dashboard** - 14 widgets  
‚úÖ **Actionable items** - Alerts ‚Üí Incidents  
‚úÖ **Security/observability signals** - Security events tracked  
‚úÖ **ML observability** - Quality metrics included  
‚úÖ **Cost optimization** - Per-model cost tracking  
‚úÖ **Production-ready** - Full deployment guide  

---

## üöÄ Next Steps for Production

1. **Deploy to Cloud**
   - Vercel / Netlify for frontend
   - Supabase Edge Functions for backend
   - Datadog Agent on servers

2. **Enable Full APM**
   - Add dd-trace server-side
   - Instrument backend services
   - Connect distributed traces

3. **Advanced ML Monitoring**
   - Integrate Perspective API for toxicity
   - Add embedding drift detection
   - Implement A/B testing for prompts

4. **Scale Observability**
   - Custom metrics to Datadog
   - Service mesh integration
   - Multi-region deployment

---

## üìû Contact

**Team**: ObservAI Engineering  
**Author**: Shafwan Safi  
**GitHub**: [@Shafwansafi06](https://github.com/Shafwansafi06)  
**Repository**: [observability-hub](https://github.com/Shafwansafi06/observability-hub)

---

## üôè Special Thanks

**Datadog** - For building the world's best observability platform  
**Google Cloud Vertex AI** - For providing cutting-edge AI capabilities  
**Open Source Community** - For the amazing tools that made this possible

---

<p align="center">
  <strong>Built with ‚ù§Ô∏è for the Datadog Hackathon 2025</strong>
</p>
