# Secure AI Gateway - Migration Guide

## ğŸ”’ Security Improvement

We've migrated from client-side AI calls to a secure backend gateway architecture.

### âš ï¸ WHY THIS CHANGE IS CRITICAL

**Before (INSECURE):**
```typescript
// âŒ DANGEROUS: API key exposed in frontend bundle
const API_KEY = import.meta.env.VITE_VERTEX_AI_API_KEY;
fetch(`https://api.google.com/...?key=${API_KEY}`);
```

**Problems:**
- `VITE_*` variables are bundled into JavaScript â†’ anyone can extract your API key
- No rate limiting â†’ attackers can abuse your quota
- No input validation â†’ prompt injection attacks possible
- No audit trail â†’ can't track who used your API
- Key rotation impossible without redeploying

**After (SECURE):**
```typescript
// âœ… SECURE: API key only on server
const response = await fetch('/api/ai/generate', {
  method: 'POST',
  body: JSON.stringify({ prompt: '...' })
});
```

**Benefits:**
- API key never leaves the server
- Rate limiting per IP address
- Input validation prevents injection
- Full audit logging
- Easy key rotation (just update env var)

---

## ğŸ“ File Structure

```
observability-hub/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ ai/
â”‚       â””â”€â”€ generate.ts          # Secure backend gateway (Vercel serverless)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ ai-client.ts         # New secure frontend client
â”‚       â””â”€â”€ vertex-ai/
â”‚           â””â”€â”€ client.ts        # Updated to use backend (backward compatible)
â””â”€â”€ .env (NOT COMMITTED)
```

---

## ğŸ”§ Environment Variables

### Server-Side (Vercel Dashboard)
Add these in **Vercel Dashboard** â†’ **Project Settings** â†’ **Environment Variables**:

```bash
# Google Vertex AI API Key (Server-side ONLY - no VITE_ prefix!)
VERTEX_AI_API_KEY=AIzaSy...your_key_here

# Optional: Supabase (already configured)
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGc...
```

### âš ï¸ CRITICAL RULES

1. **NEVER use `VITE_` prefix for secrets**
   - `VITE_*` variables â†’ bundled into client JavaScript
   - Anyone can view your bundled code and steal keys
   
2. **Server-only variables** (safe):
   - `VERTEX_AI_API_KEY` âœ…
   - `DATABASE_URL` âœ…
   - `DD_API_KEY` âœ…
   
3. **Client-safe variables** (can use `VITE_`):
   - `VITE_SUPABASE_URL` âœ… (public URL, not sensitive)
   - `VITE_SUPABASE_ANON_KEY` âœ… (public, row-level security protects data)
   - `VITE_DD_CLIENT_TOKEN` âœ… (client token, not API key)

---

## ğŸš€ Deployment Steps

### 1. Add Environment Variables to Vercel

```bash
# In Vercel Dashboard:
# Project â†’ Settings â†’ Environment Variables

Name: VERTEX_AI_API_KEY
Value: AIzaSy...your_actual_key
Environment: Production, Preview, Development
```

### 2. Remove Old Variable (if it exists)
Delete `VITE_VERTEX_AI_API_KEY` from Vercel if you added it before.

### 3. Deploy
```bash
git add .
git commit -m "feat: migrate to secure AI gateway"
git push origin main
```

Vercel will automatically deploy.

### 4. Verify

Test the endpoint:
```bash
curl -X POST https://your-app.vercel.app/api/ai/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello, world!", "model": "gemini-2.5-flash"}'
```

Expected response:
```json
{
  "text": "Hello! How can I help you today?",
  "tokens": 12,
  "latency": 456,
  "model": "gemini-2.5-flash"
}
```

---

## ğŸ’» Code Migration

### Old Code (Insecure)
```typescript
// âŒ This exposed API keys
import { vertexAI } from '@/lib/vertex-ai/client';

const response = await vertexAI.predict({
  prompt: 'Analyze this...',
  model: ModelType.TEXT_FAST
});
```

### New Code (Secure)
```typescript
// âœ… Option 1: Use new ai-client (recommended)
import { generateAIResponse } from '@/lib/ai-client';

const response = await generateAIResponse('Analyze this...', {
  model: 'gemini-2.5-flash'
});

// âœ… Option 2: Keep using vertexAI (backward compatible)
import { vertexAI } from '@/lib/vertex-ai/client';

// Still works! Internally routes through secure backend
const response = await vertexAI.predict({
  prompt: 'Analyze this...',
  model: ModelType.TEXT_FAST
});
```

**No breaking changes** - existing code continues to work!

---

## ğŸ›¡ï¸ Security Features

### 1. Rate Limiting
- **20 requests per minute per IP**
- Prevents abuse and quota exhaustion
- Returns `429` with `Retry-After` header

### 2. Input Validation
- Prompt length limits (1-32,000 chars)
- Blocks obvious injection attempts (`<script>`, `javascript:`, etc.)
- Temperature bounds (0-2)
- Token limits (1-8,192)

### 3. Error Sanitization
- API keys redacted from error messages
- Prevents accidental secret leakage in logs
- Generic errors returned to client

### 4. Demo Mode
- Works without backend when developing locally
- Returns mock responses if `VERTEX_AI_API_KEY` not set
- Allows UI testing without configuration

---

## ğŸ” Monitoring & Debugging

### Check Backend Logs
```bash
# In Vercel Dashboard:
# Deployments â†’ [your deployment] â†’ Functions â†’ /api/ai/generate

# Look for:
[AI Gateway] Request: { model: 'gemini-2.5-flash', ... }
[AI Gateway] Success: { tokens: 123, latency: '456ms' }
```

### Check Frontend Console
```javascript
// Get metrics
import { getAIMetrics } from '@/lib/ai-client';
console.log(getAIMetrics());
// {
//   totalRequests: 10,
//   successfulRequests: 9,
//   failedRequests: 1,
//   averageLatency: 523,
//   tokensUsed: 1234
// }
```

---

## ğŸ› Troubleshooting

### Error: "AI service temporarily unavailable"
**Cause:** `VERTEX_AI_API_KEY` not set in Vercel
**Fix:** Add the environment variable (see deployment steps)

### Error: "Rate limit exceeded"
**Cause:** More than 20 requests/minute from your IP
**Fix:** Wait 60 seconds, or increase limit in `api/ai/generate.ts`

### Error: "Invalid prompt content"
**Cause:** Prompt contains blocked patterns (`<script>`, etc.)
**Fix:** Remove HTML/JavaScript from your prompt

### Demo Mode Activating Unexpectedly
**Cause:** Backend can't be reached
**Fix:** 
1. Check Vercel deployment status
2. Verify `VERTEX_AI_API_KEY` is set
3. Check network tab for 503 errors

---

## ğŸ“Š Production Checklist

- [ ] `VERTEX_AI_API_KEY` added to Vercel (no `VITE_` prefix)
- [ ] Old `VITE_VERTEX_AI_API_KEY` removed from Vercel
- [ ] Deployment successful
- [ ] Test `/api/ai/generate` endpoint returns 200
- [ ] Frontend console shows no "VITE_VERTEX_AI_API_KEY not configured" warnings
- [ ] Rate limiting works (test with 21 requests in 1 minute)
- [ ] Error handling works (test with invalid prompt)
- [ ] Metrics tracking works (`getAIMetrics()` returns data)
- [ ] Demo mode disabled in production (or works as fallback)

---

## ğŸ¯ Key Takeaways

1. **Never use `VITE_` for secrets** â†’ they get bundled into client code
2. **Always use backend for AI calls** â†’ protects API keys
3. **Rate limit everything** â†’ prevents abuse
4. **Validate all inputs** â†’ prevents injection attacks
5. **Sanitize all errors** â†’ prevents secret leakage
6. **Support demo mode** â†’ enables development without full setup

---

## ğŸ“ Support

If you see errors after migration:
1. Check Vercel function logs
2. Verify environment variables
3. Test backend endpoint directly
4. Check browser console for client-side errors

**Common mistake:** Adding `VITE_VERTEX_AI_API_KEY` instead of `VERTEX_AI_API_KEY`
- `VITE_` â†’ exposed in bundle âŒ
- No prefix â†’ server-side only âœ…
